#!/usr/bin/env python3
import os
import sys
import json
import logging
from datetime import datetime, timedelta

# ---- Permite importar tu paquete api/ desde este script ----
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if ROOT_DIR not in sys.path:
    sys.path.insert(0, ROOT_DIR)
# --------------------------------------------------------------

import boto3
import psycopg2
from sqlalchemy import text
# from sqlalchemy.orm import Session # No se usa Session directamente aqu√≠, sino SessionLocal

# CAMBIO: Importar SessionLocal desde database y settings desde config
from api.app.database import SessionLocal
from api.app.config import settings # Para S3_BUCKET, AWS_REGION, DATABASE_URL si no usas os.getenv directamente

# Logging
logger = logging.getLogger("cleanup_and_archive")
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)] # Asegurar que los logs vayan a stdout para Render
)

def main():
    logger.info("üöÄ Iniciando script de limpieza y archivado de eventos...")

    # 1) Lee variables de entorno (usando el objeto settings o directamente os.getenv)
    # Usar el objeto settings es m√°s consistente si ya lo tienes configurado.
    DATABASE_URL = settings.DATABASE_URL
    S3_BUCKET    = settings.S3_BUCKET
    AWS_REGION   = settings.AWS_REGION
    # AWS_ACCESS_KEY_ID y AWS_SECRET_ACCESS_KEY son le√≠dos autom√°ticamente por boto3 del entorno.

    if not DATABASE_URL or not S3_BUCKET:
        logger.error("‚ùå Faltan variables de entorno cr√≠ticas: DATABASE_URL y/o S3_BUCKET. Terminando script.")
        sys.exit(1)

    # CAMBIO: Usar el nombre de columna correcto 'created_at'
    # El n√∫mero de d√≠as para el corte de eventos.
    # ---- MODIFICACI√ìN AQU√ç: Cambiamos el default de 1 a 7 ----
    DAYS_TO_KEEP = int(os.getenv("EVENT_RETENTION_DAYS", "7")) # Mantener 7 d√≠as por defecto
    # ---- FIN DE LA MODIFICACI√ìN ----
    cutoff_date = datetime.utcnow() - timedelta(days=DAYS_TO_KEEP)
    logger.info(f"üóìÔ∏è  Archivando y purgando eventos m√°s antiguos que: {cutoff_date.isoformat()} UTC (pol√≠tica de {DAYS_TO_KEEP} d√≠as)")


    # Nombre de la tabla de eventos (debe coincidir con models.Event.__tablename__)
    EVENTS_TABLE_NAME = "events"

    events_to_archive = []
    # 2) Abre sesi√≥n y lee filas antiguas
    try:
        with SessionLocal() as session: # Usar SessionLocal del m√≥dulo database
            logger.info(f"‚ÑπÔ∏è  Consultando eventos antiguos en la tabla '{EVENTS_TABLE_NAME}'...")
            # Usar columna 'created_at'
            result_proxy = session.execute(
                text(f"SELECT * FROM {EVENTS_TABLE_NAME} WHERE created_at < :cutoff_date ORDER BY created_at ASC"),
                {"cutoff_date": cutoff_date}
            )
            # Convertir las filas a diccionarios
            events_to_archive = [dict(row._mapping) for row in result_proxy]

            if not events_to_archive:
                logger.info("‚úÖ No hay eventos antiguos para archivar y purgar esta vez.")
                return # Salir si no hay nada que hacer

            logger.info(f"üìÑ Encontrados {len(events_to_archive)} eventos para archivar.")

            # 3) Serializa y sube a S3
            s3_file_key = f"archived_events/{datetime.utcnow().strftime('%Y-%m-%d_%H-%M-%S')}_event_archive.json"
            
            logger.info(f"üì¶ Subiendo {len(events_to_archive)} eventos a S3: s3://{S3_BUCKET}/{s3_file_key}")
            s3_client = boto3.client("s3", region_name=AWS_REGION)
            
            def datetime_converter(o):
                if isinstance(o, datetime):
                    return o.isoformat()
            
            s3_client.put_object(
                Bucket=S3_BUCKET,
                Key=s3_file_key,
                Body=json.dumps(events_to_archive, default=datetime_converter, indent=2), # A√±adido indent=2 para legibilidad en S3
                ContentType="application/json"
            )
            logger.info(f"‚úÖ Upload a S3 completado: {s3_file_key}")

            # 4) Borra de la BD (solo si la subida a S3 fue exitosa)
            logger.info(f"üóëÔ∏è  Eliminando {len(events_to_archive)} eventos archivados de la tabla '{EVENTS_TABLE_NAME}' en la base de datos...")
            event_ids_to_delete = [event['id'] for event in events_to_archive] # Asumimos que todos los eventos tienen 'id'
            
            # Necesitamos tambi√©n las 'created_at' para la clave primaria compuesta
            # Creamos una lista de tuplas (id, created_at)
            # Nota: Aseg√∫rate que los objetos 'event' en 'events_to_archive' tengan 'created_at' como objeto datetime
            # o como string ISO formateado si la consulta SQL los devuelve as√≠.
            # El json.dumps con default=datetime_converter lo maneja para S3, pero aqu√≠ lo necesitamos para la query.
            # Si `event['created_at']` ya es un string ISO, est√° bien. Si es datetime, tambi√©n.
            
            # Para borrar de forma segura con PK compuesta, es mejor iterar o usar una condici√≥n m√°s compleja
            # o borrar por una lista de IDs si 'id' es suficientemente √∫nico para este prop√≥sito de borrado.
            # Dado que usamos `PrimaryKeyConstraint('id', 'created_at')`, borrar solo por 'id' puede ser ambiguo
            # si los IDs no son globalmente √∫nicos a trav√©s de particiones.
            # Sin embargo, si `id` es un SERIAL y siempre incrementa, deber√≠a ser seguro.
            # Vamos a mantener el borrado por IDs por simplicidad, asumiendo que son los IDs correctos de los eventos archivados.

            if event_ids_to_delete:
                # Construir la condici√≥n para borrar por la clave primaria compuesta (id, created_at)
                # Esto es m√°s seguro si la tabla est√° particionada y los IDs no son √∫nicos globalmente.
                # Sin embargo, si los IDs de `events_to_archive` son los correctos, podemos usarlos.
                # La forma m√°s simple si los IDs son de la tabla principal y √∫nicos para esos eventos:
                placeholders = ", ".join([f":id_{i}" for i in range(len(event_ids_to_delete))])
                delete_statement = text(f"DELETE FROM {EVENTS_TABLE_NAME} WHERE id IN ({placeholders})")
                
                params_for_delete = {f"id_{i}": event_id for i, event_id in enumerate(event_ids_to_delete)}

                # O si quieres ser m√°s preciso con la PK compuesta, necesitar√≠as los created_at tambi√©n:
                # conditions = " OR ".join([f"(id = :id_{i} AND created_at = :created_at_{i})" for i in range(len(events_to_archive))])
                # delete_statement = text(f"DELETE FROM {EVENTS_TABLE_NAME} WHERE {conditions}")
                # params_for_delete = {}
                # for i, event_item in enumerate(events_to_archive):
                #     params_for_delete[f"id_{i}"] = event_item['id']
                #     params_for_delete[f"created_at_{i}"] = event_item['created_at'] # Asegurarse que esto sea un datetime o string compatible con la DB

                session.execute(delete_statement, params_for_delete)
                session.commit()
                logger.info(f"‚úÖ {len(event_ids_to_delete)} eventos antiguos eliminados de la base de datos.")
            else:
                logger.info("‚ÑπÔ∏è No se encontraron IDs para eliminar.")

    except Exception as e:
        logger.error(f"‚ùå Error durante el proceso de selecci√≥n o borrado de eventos / subida a S3: {e}", exc_info=True)
        # No salimos con sys.exit(1) aqu√≠ para permitir que VACUUM intente ejecutarse si es un error no cr√≠tico de S3
        # Pero si el error fue en la consulta o borrado de BD, VACUUM podr√≠a no ser ideal.
        # Considera la granularidad del manejo de errores. Por ahora, lo logueamos.

    # 5) VACUUM ANALYZE fuera de transacci√≥n y usando psycopg2 directamente para autocommit
    logger.info(f"üßπ Ejecutando VACUUM ANALYZE en la tabla '{EVENTS_TABLE_NAME}'...")
    db_conn_for_vacuum = None # Para asegurar que se cierra
    try:
        conn_string = settings.DATABASE_URL
        if conn_string.startswith("postgresql://"): # psycopg2 usa 'postgres' o nada para el esquema
            conn_string = conn_string.replace("postgresql://", "postgres://", 1)

        db_conn_for_vacuum = psycopg2.connect(conn_string)
        db_conn_for_vacuum.autocommit = True
        with db_conn_for_vacuum.cursor() as cur:
            cur.execute(f"VACUUM ANALYZE {EVENTS_TABLE_NAME};")
            logger.info(f"‚úÖ VACUUM ANALYZE en '{EVENTS_TABLE_NAME}' completado.")
    except Exception as e:
        logger.error(f"‚ùå Error durante VACUUM ANALYZE: {e}", exc_info=True)
    finally:
        if db_conn_for_vacuum:
            db_conn_for_vacuum.close()
            logger.info("‚ÑπÔ∏è Conexi√≥n de Psycopg2 (para VACUUM) cerrada.")
    
    logger.info("üéâ Script de limpieza y archivado finalizado.")

if __name__ == "__main__":
    main()